# Task 2

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r Task_2_setup, include=FALSE}
base_url <- '~/GitHub/kul-multivariate-a2/src/'
load(paste0(base_url,"spamdata.Rdata"))

library("MASS")
library(knitr)
library(tree)
library(randomForest)
library(gbm)
library(pdp)
library(heplots)
library(candisc)
library(ggplot2) # avx
library(ape)
library(dplyr)
library(kableExtra)
```
```{r Task_2_1setup, include=FALSE}
##### FUNCTIONS #####
err<-function(observed,predicted){
  tab<-table(observed,predicted)
  #print(tab)
  err<-1-sum(diag(tab))/sum(tab)
  #return(err)
  newList <- list("tab" = tab, "err" = err)
}
```

## Introduction

  For task 2, we are going to deal with a dataset containing information about $4601$ webmails. We have $48$ variables describing the frequency of some specific words like _“remove”_ in each observation, $6$ variables describing the frequency of some specific chars like _“$”_ in one observation, and three variables, _capital_run_length_longest_, _capital_run_length_average_ and _capital_run_length_total_, describing the length of the longest uninterrupted sequence of capital letters, the average length of uninterrupted sequences of capital letters, and the total number of capital letters in each observation respectively. We also have a variable called _spam_, which indicates whether this webmail is a spam with $0$ and $1$, where $1$ for spam, and $0$ for not spam. Here all our variables are numeric type.
Our task is to use these $57$ attribute variables to classify whether a webmail is spam.

## Methodology
In order to validate the accuracy of our methods, we firstly divide our dataset into a train set, which contains 2500 observations, and a test set, which contains 2101 observations. We use the train set to train our models, and then apply it to the test set to validate its accuracy.

## Results
### 1. Classification Trees
In this part, we are going to discuss the results obtained by complex tree model and pruned tree model.
We begin with construct a complex tree model by dividing our observation into small non-overlapping regions according to some numerical criteria. Here we split our dataset until each leaf of our classification tree contains only less then 2 observations. The method used here is recursive binary splitting.

```{r Task_2_2, include=FALSE}
#create train and test set
set.seed(0829539)
trainsize<-2500 #2500
#nrotree <- 500 --> no va 
#trainsize<-250
nrotree <- 5000
train = sample(nrow(spamdata), trainsize)
data.train<-spamdata[train,]
data.test<-spamdata[-train,]

```
```{r Task_2_3, fig.height=9, fig.width=8, background=c(0,.5,0), comment=NA}
#grow complex tree using deviance as criterion
tree.mod = tree(factor(spam) ~ . ,data.train, 
                control = tree.control(nobs = 2500, minsize = 2, mincut = 1),
                split = "deviance")

summary(tree.mod)

#plot tree
plot(tree.mod)
text(4, 7, expression(bar(x) == sum(frac(x[i], n), i==1, n)))
#rpart.plot(tree.mod)
text(tree.mod,pretty=10,cex=0.65)
```

  We can see clearly here that criteria concerning the frequency of “$”, “remove”, “!”,  “hp”, “our”, “free”,  “edu” as well as the length of the longest uninterrupted sequence of capital letters are used for splitting. It’s actually quite reasonable, because from our own experience, spam webmails are always advertisements on money related topics or education related topics, and are always filled with words in capital letters, together with exclamation symbols, to draw attention.

  Since the process of recursive binary splitting may lead to overfitting, where we obtain a complex tree with a good fit on the training data, but with a poor performance on test data, we introduce the process of tree pruning. Actually, a smaller tree with fewer splits may have a lower variance at the cost of acceptable little bias.
  In order to decide the optimal tuning parameter which leads to both much lower variance and acceptable bias, we use cross-validation to make a selection. In fact, the least cross-validation error implies the least probability of overfitting, as it’s also a train-test process.

```{r Task_2_4, fig.height=8, fig.width=8, comment=NA, out.width='90%', results='asis'}
  #use cross-validation to select tuning parameter for pruning the tree
  set.seed(0829539)
  cv.out=cv.tree(tree.mod,K=5)
  par(cex=1.4)
  plot(cv.out$size,cv.out$dev,type='b')
```

  However, in this specific task, we can see that the cross-validation error is monotonously decreasing, so the optimal fold number is the original fold number. We choose best size=12 here such that the pruned tree model here is exactly the same as our complex tree model.
  
```{r Task_2_5, comment=NA, eval=TRUE}
#prune the tree
prune.mod=prune.tree(tree.mod,best=12)
plot(prune.mod)
text(prune.mod,pretty=10, cex=0.65)

```

  Now we validate the accuracy of our classification tree model with the test data. 
  
```{r Task_2_6, comment=NA, include=TRUE}
#make predictions on training and test set using the unpruned tree
pred.train<- predict(tree.mod,newdata=data.train)
classif.train<-ifelse(pred.train[,2]>=pred.train[,1],1,0)
t26 <- err(data.train$spam,classif.train)
kbl(t26$tab)
```
```{r Task_2_7, comment=NA, include=TRUE}
#make predictions on training and test set using the unpruned tree
pred.test<-predict(tree.mod,newdata=data.test)
classif.test<-ifelse(1*pred.test[,2]>=pred.test[,1],1,0)
t27 <- err(data.test$spam,classif.test)
kbl(t27$tab)
```
```{r Task_2_8, comment=NA}
#make predictions on training and test set using the pruned tree
pred.train<-predict(prune.mod,newdata=data.train)
classif.train<-ifelse(pred.train[,2]>=pred.train[,1],1,0)
t28 <- err(data.train$spam,classif.train)
kbl(t28$tab)
```
```{r Task_2_9, comment=NA}
#make predictions on training and test set using the pruned tree
pred.test<-predict(prune.mod,newdata=data.test)
classif.test<-ifelse(1*pred.test[,2]>=pred.test[,1],1,0)
t29 <- err(data.test$spam,classif.test)
kbl(t29$tab)
```

  We can conclude that our classification model performs very well. since the complex tree is the same as pruned tree here, we can see that the test error is just very slightly higher than the train error. There is not much overfitting here.

### 2. Different Classification Models
  Now for this part, we are going to compare different classification models using two different classification scenarios.

  Firstly, we consider the scenario where we have different prior probabilities and equal classification costs. We use the entire dataset to figure out the prior probability.

```{r Task_2_10, comment=NA}  
#(a)Account for different prior probabilities and equal classification costs 
# (1) linear discriminant analysis
set.seed(0829539)
ldaS.out<-lda(spam ~ .,data = spamdata)
print(ldaS.out$prior)
```

  So, we take $P$ (is spam)=0.394, P (not spam) =0.606 as our prior probability, and we can verify that their sum equals 1.

  The four classification models we are going to compare is 1) Linear Discriminant Analysis, 2) Bagging, 3) Random Forests and 4) Gradient Boosting. Since the dimension here is very high and the mathematical assumption of our dataset is very weak, we do not suppose that Linear Discriminant Analysis without Principal Component analysis here will have very good performance. We just take it as a baseline. The three other methods are all methods used to improve the classification tree model, as tree models are always with high variance, thus high probability to cause overfitting.

The results are listed as below:

#### 1) Linear Discriminant Analysis

```{r Task_2_11, comment=NA}  
lda.out<-lda(spam~.,prior=c(0.606,0.394),data=data.train)

scaling <- lda.out$scaling %>% as.data.frame() %>% mutate(name = rownames(lda.out$scaling))
scaling %>% filter(name %in% c("char_freq_dollar", "word_freq_remove", "word_freq_table",
                               "word_freq_conference", "char_freq_dotcomma", 
                               "char_freq_parenthesis", "char_freq_bracket", "char_freq_exclmark" ))
```

  Here we can see that the three attributes highly correlated with spam webmails the frequency of “$”, “!” and “remove”, are exactly the same as what we have got from our classification tree model. What is interesting here is we can also see that the frequency of “table”, “;” and “parenthesis” shows strong negative correlation with spam webmails.

```{r Task_2_12, comment=NA}  
pred.train<-predict(lda.out,data.train)
t212 <- err(data.train$spam,pred.train$class)
kbl(t212$tab)
```
```{r Task_2_13, comment=NA}  
pred.test<-predict(lda.out,data.test)
t213 <- err(data.test$spam,pred.test$class)
kbl(t213$tab)
```

#### 2) Bagging

  just some text

```{r Task_2_14N, comment=NA, warning=FALSE, include=FALSE}
load(paste0(base_url,"T2/T2_1_bagging.Rdata"))
bag.mod <- t2_bagging_1
```
```{r Task_2_14Y1, comment=NA, warning=FALSE, eval=FALSE}  
#(2) Bagging
set.seed(0829539)
bag.mod=randomForest(spam ~ ., data = data.train, 
                     classwt=c(0.606,0.394),
                     mtry=57, ntree=nrotree, importance=TRUE)
bag.mod
```
```{r Task_2_14Y2, comment=NA,echo=FALSE}
bag.mod
```
```{r Task_2_15, include=FALSE}  
par(cex=1.2)
plot(1:3500,bag.mod$err.rate[1:3500,1], xlab="Number of iterations", 
     ylab="OOB error", ylim=c(0.08,0.2), 
     pch='', main="OOB error")
lines(1:3500,bag.mod$err.rate[1:3500,1],col="red")
```
```{r Task_2_16, comment=NA}  
#importance(bag.mod,plot=TRUE)
varImpPlot(bag.mod, type = 2, cex = 0.6)

```
```{r Task_2_17, include=FALSE}  
par(cex=1.2)
plot(1:3500,bag.mod$err.rate[1:3500,1], xlab="Number of iterations", 
     ylab="OOB error", ylim=c(0.08,0.2), 
     pch='', main="OOB error")
lines(1:3500,bag.mod$err.rate[1:3500,1],col="red")
```
```{r Task_2_18, comment=NA}  
pred.train<-predict(bag.mod,newdata=data.train)
t218 <- err(data.train$spam,pred.train)
kbl(t218$tab)

```
```{r Task_2_19, comment=NA}  
pred.test<-predict(bag.mod,newdata=data.test)
t219 <- err(data.test$spam,pred.test)
#kbl(t219$tab)
```

  Actually, with the MeanDecreaseGini graph we can see that the frequency of “$”, “!” and “remove” made evidently the most contribution. This is the same conclusion we have drawn from our classification tree model and Linear Discriminant Analysis. Here Out Of Bag error rate is 6.76%, which is acceptable.

#### 3) Random Forests
  just some text.

```{r Task_2_20N, comment=NA, warning=FALSE, include=FALSE}
load(paste0(base_url,"T2/T2_ranfor_2.Rdata"))
rf.mod<-t2_ranfor_2
```
```{r Task_2_20Y1, comment=NA, eval=FALSE}
# (3) random forests
set.seed(0829539)
rf.mod=randomForest(spam ~ ., data = data.train, 
                    classwt=c(0.606,0.394), mtry=5, 
                    ntree=5000, importance=TRUE)
rf.mod

```
```{r Task_2_20Y2, comment=NA,echo=FALSE}
rf.mod
```
```{r Task_2_21Y1, comment=NA}
pred.train<-predict(rf.mod,newdata=data.train)
t2_21Y1 <- err(data.train$spam,pred.train)
t2_21Y1$tab
```
```{r Task_2_21Y2, comment=NA}
pred.test<-predict(rf.mod,newdata=data.test)
t2_21Y2 <-err(data.test$spam,pred.test)
t2_21Y2$tab
```
```{r Task_2_22, fig.height=8, fig.width=10, comment=NA}
varImpPlot(rf.mod, cex = 0.6) 
#varImpPlot(bag.mod, type = 2, cex = 0.6)
```

  Random Forests Model is also based on the idea of decorrelating trees. Here we choose m=5 for the size of our predictor set. can see that although there is slightly difference, “$”, “!” and “remove” still stand for spam webmails, as well as the length of the longest uninterrupted sequence of capital letters. 

#### 4)	Gradient Boosting

  Different from Bagging and Random Forests, the number of trees for Gradient Boosting is not expected to be as large as possible, since using too many trees may cause overfitting for Gradient Boosting. We start with deciding the optimal number of trees by cross-validation, for the same reason explained before.

  Note that since we are going to suppose Bernoulli distribution in our gbm function, we should firstly make sure that our variable for classification is numeric type.

```{r Task_2_23Y1, comment=NA}
# (4) gradient boosting
set.seed(0829539)
data.train$spam<-ifelse(data.train$spam=="1_yes",1,0)
data.test$spam<-ifelse(data.test$spam=="1_yes",1,0)
spamdata$spam<-ifelse(spamdata$spam=="1_yes",1,0)
```
```{r Task_2_23N1, comment=NA}
load(paste0(base_url,"T2/T2_gbm_3.Rdata"))
boost.mod<-t2_gbm_3
```
```{r Task_2_23Y2, comment=NA, eval=FALSE}
#use distribution="bernoulli" for binary target
#interaction.depth=4, means that we fit a tree that uses 4 splits 
#(and that includes at most a 4-th order interaction)
boost.mod=gbm(spam ~ . ,data = data.train, distribution = "bernoulli", 
              n.trees = 20000, interaction.depth = 4, shrinkage = 0.001, cv.folds = 5)
gbm.perf(boost.mod,method="cv")
legend("topright",c("train error","CV error"),col=c("green","black"),lty=c(1,1))
```
```{r Task_2_23Y3, comment=NA, eval=TRUE, echo=FALSE}
gbm.perf(boost.mod,method="cv")
legend("topright",c("train error","CV error"),col=c("green","black"),lty=c(1,1))
```

So here we get that the optimal number of trees is 19932. We use this parameter to continue our analysis.

```{r Task_2_24, comment=NA, eval=TRUE}
#relative influence plot
par(cex=1.2)
summary(boost.mod,n.trees=19932)
```

  We can actually get nearly the same result as we obtained from all the former models concerning the variable most correlated to spam webmails here, that the frequency of “$”, “!” and “remove” come top.
  
```{r Task_2_25, comment=NA, eval=TRUE}
pred.train<-predict(boost.mod,n.trees=19932,newdata=data.train,type="response")
classif.train<-ifelse(pred.train>=1-pred.train,1,0)
t225 <- err(data.train$spam,classif.train)
t225$tab
```
```{r Task_2_26, comment=NA, eval=TRUE}
pred.test<-predict(boost.mod,n.trees=19932,newdata=data.test,type="response")
classif.test<-ifelse(pred.test>=1-pred.test,1,0)
t226 <- err(data.test$spam,classif.test)
t226$tab
```

  Then, we consider the scenario where we have different prior probabilities and unequal classification costs: C(spam|email)=10*C(email|spam). Actually, the models here remain the same, and we just need to do some reclassification based on the posterior probabilities obtained from our built models and a criterion taking the classification costs as weights for posterior probabilities.

##### 1) Linear Discriminant Analysis
  **some text here**
```{r Task_2_27N1, comment=NA, eval=TRUE, include=FALSE}
set.seed(0829539)
data.train$spam<-factor(ifelse(data.train$spam==1,"1_yes","0_no"))
data.test$spam<-factor(ifelse(data.test$spam==1,"1_yes","0_no"))
spamdata$spam<-factor(ifelse(spamdata$spam==1,"1_yes","0_no"))

load(paste0(base_url,"T2/T2_ldaout_7.Rdata"))
lda.out<-t2_ldaout_7

```
```{r Task_2_27Y1, comment=NA, eval=FALSE, include=FALSE}
lda.out<-lda(spam~.,prior=c(0.606,0.394),data=data.train)
print(lda.out)
```
```{r Task_2_27N2, comment=NA, eval=TRUE, include=FALSE}
print(lda.out)
```
```{r Task_2_27Y2, comment=NA, eval=TRUE, include=TRUE}
pred.train<-predict(lda.out,data.train)
classif.train<-ifelse(1*pred.train$posterior[,2]>=10*pred.train$posterior[,1],1,0)
t227Y2 <- err(data.train$spam, classif.train)
t227Y2$tab
```
```{r Task_2_27Y3, comment=NA, eval=TRUE, include=TRUE}
pred.test<-predict(lda.out,data.test)
classif.test<-ifelse(1*pred.test$posterior[,2]>=10*pred.test$posterior[,1],1,0)
t227Y3 <- err(data.test$spam,classif.test)
t227Y3$tab
```
##### 2) bagging
  **some text here**
```{r Task_2_28N1, comment=NA, eval=TRUE, include=FALSE}

load(paste0(base_url,"T2/T2_bagmod_4.Rdata"))
bag.mod <- t2_bagmod_4
```
```{r Task_2_28N2, comment=NA, eval=FALSE, include=FALSE}
# (2) bagging

set.seed(0829539)
bag.mod=randomForest(spam~.,data=data.train,mtry=57,ntree=5000,classwt=c(0.606,0.394),importance=TRUE)
bag.mod
#plot oob error
par(cex=1.2)
plot(1:3500,bag.mod$err.rate[1:3500,1],xlab="Number of iterations",ylab="OOB error",ylim=c(0.08,0.2),pch='',main="OOB error")
lines(1:3500,bag.mod$err.rate[1:3500,1],col="red")
#plot variable importance
importance(bag.mod,plot=TRUE)
varImpPlot(bag.mod,type=2,cex=1.2)

```
```{r Task_2_28Y1, comment=NA, eval=TRUE, include=TRUE}

pred.train<-predict(bag.mod,type="prob",newdata=data.train)
classif.train<-ifelse(1*pred.train[,2]>=10*pred.train[,1],1,0)
t228Y1 <- err(data.train$spam,classif.train)
t228Y1$tab

```
```{r Task_2_28Y2, comment=NA, eval=TRUE, include=TRUE}
pred.test<-predict(bag.mod,type="prob",newdata=data.test)
classif.test<-ifelse(1*pred.test[,2]>=10*pred.test[,1],1,0)
t228Y2 <- err(data.test$spam,classif.test)
t228Y2$tab
```

##### 3) Random Forests
  **some text here**
```{r Task_2_29N1, comment=NA, eval=TRUE, include=FALSE}
load(paste0(base_url,"T2/T2_rfmod_5.Rdata"))
rf.mod <- t2_rfmod_5
```
```{r Task_2_29N2, comment=NA, eval=FALSE, include=FALSE}
# (3) random forests
set.seed(0829539)
rf.mod=randomForest(spam~.,data=data.train,mtry=5,ntree=5000,classwt=c(0.606,0.394),importance=TRUE)
rf.mod
```
```{r Task_2_29Y1, comment=NA, eval=TRUE, include=TRUE}
pred.train<-predict(rf.mod,type="prob",newdata=data.train)
classif.train<-ifelse(1*pred.train[,2]>=10*pred.train[,1],1,0)
t229Y1 <- err(data.train$spam,classif.train)
t229Y1$tab
```
```{r Task_2_29Y2, comment=NA, eval=TRUE, include=TRUE}
pred.test<-predict(rf.mod,type="prob",newdata=data.test)
classif.test<-ifelse(1*pred.test[,2]>=10*pred.test[,1],1,0)
t229Y2 <- err(data.test$spam,classif.test)
t229Y2$tab
```
```{r Task_2_29N3, comment=NA, eval=FALSE, include=FALSE}
#variable importance plot
#two measures are reported: 
#increase in MSE computed on OOB samples when removing variable
#decrease in node impurity (or increase in node purity) resulting from splits on the variable
importance(rf.mod)
varImpPlot(rf.mod)
```

4) Gradient Boosting
  **some text here**
```{r Task_2_30N1, comment=NA, eval=TRUE, include=FALSE}
load(paste0(base_url,"T2/T2_boostmod_6.Rdata"))
boost.mod <- t2_boostmod_6
```
```{r Task_2_30N2, comment=NA, eval=FALSE, include=FALSE}
set.seed(0829539)
data.train$spam<-ifelse(data.train$spam=="1_yes",1,0)
data.test$spam<-ifelse(data.test$spam=="1_yes",1,0)
spamdata$spam<-ifelse(spamdata$spam=="1_yes",1,0)

#use distribution="bernoulli" for binary target
#interaction.depth=4, means that we fit a tree that uses 4 splits 
#(and that includes at most a 4-th order interaction)
boost.mod=gbm(spam~.,data=data.train,distribution="bernoulli",n.trees=20000,interaction.depth=4,shrinkage=0.001,cv.folds=5)
boost.mod_save = boost.mod
gbm.perf_save = gbm.perf(boost.mod,method="cv")
legend("topright",c("train error","CV error"),col=c("green","black"),lty=c(1,1))
#relative influence plot
par(cex=1.2)
summary(boost.mod,n.trees=19932)

```
```{r Task_2_30Y1, comment=NA, eval=TRUE, include=TRUE}
pred.train<-predict(boost.mod,n.trees=19932,newdata=data.train,type="response")
classif.train<-ifelse(1*pred.train>=10*(1-pred.train),1,0)
t230Y1 <- err(data.train$spam,classif.train)
t230Y1$tab
```
```{r Task_2_30Y2, comment=NA, eval=TRUE, include=TRUE}
pred.test<-predict(boost.mod,n.trees=19932,newdata=data.test,type="response")
classif.test<-ifelse(1*pred.test>=10*(1-pred.test),1,0)
t230Y2 <- err(data.test$spam,classif.test)
t230Y2$tab
```

  We arrange all the training errors and test errors into a whole table below, and calculate the sensitivities as well as the false positive rates for the test set, using the probability tables we printed out:

\begin{table}[h!]
  \begin{center}
    \caption{}
    \label{tab:}
    \begin{tabular}{c|c|c|c|c}
      \textbf{ - } & \textbf{training error} & \textbf{test error} & \textbf{sensitivity} & \textbf{false positive rate}\\
      \hline
        a)LDC&0.1076&0.1195&0.7679&0.0444\\
        a)Bagging&0&0.0643&0.8869&0.0317\\
        a)Random Forests&0.0036&0.0505&0.9143&0.0270\\
        a)Gradient Boosting&0.0216&0.0571&0.9131&0.0373\\
        b)LDC&0.2128&0.2285&0.4452&0.0111\\
        b)Bagging&0.07&0.1290&0.6893&0.0008\\
        b)Random Forests&0.0872&0.1856&0.5393&0.0002\\
        b)Gradient Boosting&0.068&0.1066&0.7440&0.0007\\
    \end{tabular}
  \end{center}
\end{table}
  We can see that as we claimed at the beginning, Linear Discriminant Analysis performs the worst in both scenarios, Random Forests performs the best in scenario a), where we have different prior probabilities but equal classification costs, and Gradient Boosting performs the best in scenario b), where we have different prior probabilities and unequal classification costs. Generally, for scenario a), the performance of Random Forests and Gradient Boosting is equally good, and Bagging also performs good, just with slightly higher test error than Random Forests and Gradient Boosting. However, for scenario b), the accuracy of Random Forests decreases much faster than Bagging. 

  What’s more, as is expected, in scenario b), since misclassifying a not spam webmail into spam ones costs much more than misclassifying a spam webmail into not spam ones, our models all tend to classify more cases as not spam ones to minimize the cost. As a result, the false positive rate is very low for all models, but the accuracy and sensitivity decrease significantly for all models, since a number of real positive cases are classified as not spam webmails. Gradient Boosting and Bagging are relatively more stable here compared with the other two models.
